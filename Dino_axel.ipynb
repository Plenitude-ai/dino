{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dino"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to see the attention maps of a photo example\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Image\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c94080d8b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"\"\"C:\\\\Users\\\\axeld\\\\Pictures\\\\selfie_coiffeur_28.08.21.jpeg\"\"\"\r\n",
    "img = Image.open(path)\r\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Attention Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Visualize Self-Attention maps [-h]\n",
      "                                     [--arch {vit_tiny,vit_small,vit_base}]\n",
      "                                     [--patch_size PATCH_SIZE]\n",
      "                                     [--pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                                     [--checkpoint_key CHECKPOINT_KEY]\n",
      "                                     [--image_path IMAGE_PATH]\n",
      "                                     [--image_size IMAGE_SIZE [IMAGE_SIZE ...]]\n",
      "                                     [--output_dir OUTPUT_DIR]\n",
      "                                     [--threshold THRESHOLD]\n",
      "Visualize Self-Attention maps: error: argument -h/--help: ignored explicit argument 'elp'\n"
     ]
    }
   ],
   "source": [
    "!python visualize_attention.py -help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Visualize Self-Attention maps [-h]\n",
      "                                     [--arch {vit_tiny,vit_small,vit_base}]\n",
      "                                     [--patch_size PATCH_SIZE]\n",
      "                                     [--pretrained_weights PRETRAINED_WEIGHTS]\n",
      "                                     [--checkpoint_key CHECKPOINT_KEY]\n",
      "                                     [--image_path IMAGE_PATH]\n",
      "                                     [--image_size IMAGE_SIZE [IMAGE_SIZE ...]]\n",
      "                                     [--output_dir OUTPUT_DIR]\n",
      "                                     [--threshold THRESHOLD]\n",
      "Visualize Self-Attention maps: error: unrecognized arguments: #\n"
     ]
    }
   ],
   "source": [
    "# !python visualize_attention.py --output_dir \"./output\" --image_path $path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please use the `--pretrained_weights` argument to indicate the path of the checkpoint to evaluate.\n",
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n",
      "Please use the `--image_path` argument to indicate the path of the image you wish to visualize.\n",
      "Since no image path have been provided, we take the first image in our paper.\n",
      "./output_base\\attn-head0.png saved.\n",
      "./output_base\\attn-head1.png saved.\n",
      "./output_base\\attn-head2.png saved.\n",
      "./output_base\\attn-head3.png saved.\n",
      "./output_base\\attn-head4.png saved.\n",
      "./output_base\\attn-head5.png saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\axeld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3609: UserWarning: Default upsampling behavior when mode=bicubic is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  warnings.warn(\n",
      "C:\\Users\\axeld\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\functional.py:3657: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "!python visualize_attention.py --output_dir \"./output_base\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c95236fb80>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\r\n",
    "\r\n",
    "img0 = Image.open(\"output/attn-head0.png\")\r\n",
    "img1 = Image.open(\"output/attn-head1.png\")\r\n",
    "img2 = Image.open(\"output/attn-head2.png\")\r\n",
    "img3 = Image.open(\"output/attn-head3.png\")\r\n",
    "img4 = Image.open(\"output/attn-head4.png\")\r\n",
    "img5 = Image.open(\"output/attn-head5.png\")\r\n",
    "\r\n",
    "\r\n",
    "fig, ax = plt.subplots(2,3, figsize=(15,15))\r\n",
    "ax[0,0].imshow(img0)\r\n",
    "ax[0,1].imshow(img1)\r\n",
    "ax[0,2].imshow(img2)\r\n",
    "ax[1,0].imshow(img3)\r\n",
    "ax[1,1].imshow(img4)\r\n",
    "ax[1,2].imshow(img5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c953cb4400>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img0_ = np.array(img0)[:,:,:3]\r\n",
    "img1_ = np.array(img1)[:,:,:3]\r\n",
    "img2_ = np.array(img2)[:,:,:3]\r\n",
    "img3_ = np.array(img3)[:,:,:3]\r\n",
    "img4_ = np.array(img4)[:,:,:3]\r\n",
    "img5_ = np.array(img5)[:,:,:3]\r\n",
    "\r\n",
    "img_ = ((img0_ + img1_ + img2_ + img3_ + img4_ + img5_ )/6).astype(np.uint8)\r\n",
    "\r\n",
    "fig = plt.figure(figsize=(10,10))\r\n",
    "plt.imshow(img_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f17faad1cc96198557072074a1d17b9b433901c680f8aaef8a19317e41007560"
  },
  "kernelspec": {
   "display_name": "dino",
   "language": "python",
   "name": "dino"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
